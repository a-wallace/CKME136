{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carbon Tax Sentiment Analysis - Initial Code and Results\n",
    "\n",
    "To begin the creation of a twitter sentiment analyzer, there are four steps that must be accomplished:\n",
    "\n",
    "1. Importing libraries and dataset\n",
    "2. Initial Data Cleaning\n",
    "3. Dividing data into Training and test sets\n",
    "4. Model prediction and evaluation\n",
    "\n",
    "After these steps have been accomplished, an initil starting point will be established from which the model can be improved by experimenting with different pre-processing techniques and machine learning algorithms. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Libraries and Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "'exec(%matplotlib inline)'\n",
    "\n",
    "# import dataset\n",
    "\n",
    "data_source_url = \"https://github.com/a-wallace/CKME136/blob/master/carbon%20tax%20tweets.csv\"\n",
    "carbon_tweets = pd.read_csv(data_source_url)\n",
    "\n",
    "nltk.download('stopwords')\t# ensure nltk stopword database is present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initial Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING \n",
    "\n",
    "# remove all special characters\n",
    "carbon_tweets['Tweet'] =  [re.sub(r'\\W', ' ', str(x)) for x in carbon_tweets['Tweet']]\n",
    "\n",
    "# remove all single characters\n",
    "carbon_tweets['Tweet'] =  [re.sub(r'\\+[a-zA-Z]\\s+', ' ', str(x)) for x in carbon_tweets['Tweet']]\n",
    "\n",
    "# remove single characters from the start\n",
    "carbon_tweets['Tweet'] =  [re.sub(r'\\^[a-zA-Z]\\s+', ' ', str(x)) for x in carbon_tweets['Tweet']]\n",
    "\n",
    "# substituting multiple spaces with single space\n",
    "carbon_tweets['Tweet'] =  [re.sub(r'\\s+', ' ', str(x)) for x in carbon_tweets['Tweet']]\n",
    "\n",
    "# removing prefixed 'b'\n",
    "carbon_tweets['Tweet'] =  [re.sub(r'^b\\s+', ' ', str(x)) for x in carbon_tweets['Tweet']]\n",
    "\n",
    "# converting to lowercase\n",
    "carbon_tweets['Tweet'] =  [x.lower() for x in carbon_tweets['Tweet']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Dividing data into Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide data into training and tests sets\t\n",
    "\n",
    "dependent_vars = carbon_tweets['Tweet']\n",
    "independent_vars = carbon_tweets[\"Polarity\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dependent_vars, independent_vars, test_size=0.2, random_state=0)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "training_data_transformed = vectorizer.fit_transform(X_train)\n",
    "testing_data_transformed = vectorizer.transform(X_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "text_classifier.fit(training_data_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions / evaluating model\n",
    "\n",
    "predictions = text_classifier.predict(testing_data_transformed)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"ACCURACY SCORE:\")\n",
    "print(accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "\n",
    "The model performs poorly with an accuracy of 50%. This is to be expected, given the extremely small number of observations present in the dataset and the minimal text pre-processing that took place. There is high potential to vastly improve the accuracy of the model by doing three things. First, increase the dataset size. I have figured out the data limitation the Twitter API places on users and will implement a daatset for the final project with 3,000 observations. This will strenghten the testing and training data. Second, implement more advanced text pre-processing steps learned from the literature review. \n",
    "Third, experiment with different NLP machine learning algorithms. Random Forest was used here, but it will be interesting to see the difference in accuracy when Naive Bayes is used or some other algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
