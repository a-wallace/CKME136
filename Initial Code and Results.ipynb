{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carbon Tax Sentiment Analysis - Inital Results and Code\n",
    "\n",
    "To begin the process of creating a sentiment analyzer there are four steps to be followed. \n",
    "\n",
    "1. Import libraries and Dataset\n",
    "2. Initial Data Cleaning\n",
    "3. Split Data into Training and Test\n",
    "4. Prediction and Model Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aidan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "'exec(%matplotlib inline)'\n",
    "\n",
    "# import dataset\n",
    "\n",
    "data_source_url = r\"carbon_tax_tweets.csv\"\n",
    "carbon_tweets = pd.read_csv(data_source_url)\n",
    "\n",
    "# ensure nltk stopword database is present\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initial Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING \n",
    "\n",
    "# remove all special characters\n",
    "carbon_tweets['Tweet'] =  [re.sub(r'\\W', ' ', str(x)) for x in carbon_tweets['Tweet']]\n",
    "\n",
    "# remove all single characters\n",
    "carbon_tweets['Tweet'] =  [re.sub(r'\\+[a-zA-Z]\\s+', ' ', str(x)) for x in carbon_tweets['Tweet']]\n",
    "\n",
    "# remove single characters from the start\n",
    "carbon_tweets['Tweet'] =  [re.sub(r'\\^[a-zA-Z]\\s+', ' ', str(x)) for x in carbon_tweets['Tweet']]\n",
    "\n",
    "# substituting multiple spaces with single space\n",
    "carbon_tweets['Tweet'] =  [re.sub(r'\\s+', ' ', str(x)) for x in carbon_tweets['Tweet']]\n",
    "\n",
    "# removing prefixed 'b'\n",
    "carbon_tweets['Tweet'] =  [re.sub(r'^b\\s+', ' ', str(x)) for x in carbon_tweets['Tweet']]\n",
    "\n",
    "# converting to lowercase\n",
    "carbon_tweets['Tweet'] =  [x.lower() for x in carbon_tweets['Tweet']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# divide data into training and tests sets\n",
    "\n",
    "dependent_vars = carbon_tweets['Tweet']\n",
    "independent_vars = carbon_tweets[\"Polarity\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dependent_vars, independent_vars, test_size=0.2, random_state=0)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "training_data_transformed = vectorizer.fit_transform(X_train)\n",
    "testing_data_transformed = vectorizer.transform(X_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "text_classifier.fit(training_data_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Predictions/Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX:\n",
      "[[10  1  0]\n",
      " [ 9  1  0]\n",
      " [ 1  0  0]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.50      0.91      0.65        11\n",
      "           0       0.50      0.10      0.17        10\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50        22\n",
      "   macro avg       0.33      0.34      0.27        22\n",
      "weighted avg       0.48      0.50      0.40        22\n",
      "\n",
      "ACCURACY SCORE:\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aidan\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions = text_classifier.predict(testing_data_transformed)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"ACCURACY SCORE:\")\n",
    "print(accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Next Steps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The model performs poorly with an accuracy of 50%. The dataset is to small to be properly trained on. As a result, the data rate limit problem imposed by the Twitter API must be fixed so more data can be added. To improve the accuracy further, the inital data cleaning must be expanded upon to include the text pre-processing methods learned from the literature review. Also, the Random Forest method is used here but different machine learning algorithms such as Naive Bayes hold the potential to increase accuracy further. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
